{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import (\n",
    "    LlamaCppEmbeddings, \n",
    "    HuggingFaceEmbeddings, \n",
    "    SentenceTransformerEmbeddings\n",
    ")\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "from langchain.document_loaders import (\n",
    "    PyPDFLoader,\n",
    "    DataFrameLoader,\n",
    "    GitLoader\n",
    "  )\n",
    "\n",
    "import streamlit as st\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import base64\n",
    "import textwrap\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings,HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma,FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "import pandas as pd\n",
    "import nbformat\n",
    "from nbconvert import PythonExporter\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_text_splits(text_file):\n",
    "  \"\"\"Function takes in the text data and returns the  \n",
    "  splits so for further processing can be done.\"\"\"\n",
    "  with open(text_file,'r') as txt:\n",
    "    data = txt.read()\n",
    "\n",
    "  textSplit = RecursiveCharacterTextSplitter(chunk_size=150,\n",
    "                                             chunk_overlap=15,\n",
    "                                             length_function=len)\n",
    "  doc_list = textSplit.split_text(data)\n",
    "  return doc_list\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loader = PyPDFLoader(\"The-Holy-Bible-King-James-Version.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1342\n",
      "The King James Version of the\n",
      "Holy Bible\n",
      "Downloaded from www.holybooks.com\n",
      "www.holybooks.com\n"
     ]
    }
   ],
   "source": [
    "print(len(pages))\n",
    "print(pages[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_pdf_splits(pdf_file):\n",
    "  \"\"\"Function takes in the pdf data and returns the  \n",
    "  splits so for further processing can be done.\"\"\"\n",
    "  \n",
    "  loader = PyPDFLoader(pdf_file)\n",
    "  pages = loader.load_and_split()  \n",
    "\n",
    "  textSplit = RecursiveCharacterTextSplitter(chunk_size=150,\n",
    "                                             chunk_overlap=15,\n",
    "                                             length_function=len)\n",
    "  doc_list = []\n",
    "  #Pages will be list of pages, so need to modify the loop\n",
    "  for pg in pages:\n",
    "    pg_splits = textSplit.split_text(pg.page_content)\n",
    "    doc_list.extend(pg_splits)\n",
    "\n",
    "  return doc_list\n",
    "\n",
    "\n",
    "def get_excel_splits(excel_file,target_col,sheet_name):\n",
    "  trialDF = pd.read_excel(io=excel_file,\n",
    "                          engine='openpyxl',\n",
    "                          sheet_name=sheet_name)\n",
    "  \n",
    "  df_loader = DataFrameLoader(trialDF,\n",
    "                              page_content_column=target_col)\n",
    "  \n",
    "  excel_docs = df_loader.load()\n",
    "\n",
    "  return excel_docs\n",
    "\n",
    "\n",
    "def get_csv_splits(csv_file):\n",
    "  \"\"\"Function takes in the csv and returns the  \n",
    "  splits so for further processing can be done.\"\"\"\n",
    "  csvLoader = CSVLoader(csv_file)\n",
    "  csvdocs = csvLoader.load()\n",
    "  return csvdocs\n",
    "\n",
    "\n",
    "def get_ipynb_splits(notebook):\n",
    "  \"\"\"Function takes the notebook file,reads the file \n",
    "  data as python script, then splits script data directly\"\"\"\n",
    "\n",
    "  with open(notebook) as fh:\n",
    "    nb = nbformat.reads(fh.read(), nbformat.NO_CONVERT)\n",
    "\n",
    "  exporter = PythonExporter()\n",
    "  source, meta = exporter.from_notebook_node(nb)\n",
    "\n",
    "  #Python file data is in the source variable\n",
    "  \n",
    "  textSplit = RecursiveCharacterTextSplitter(chunk_size=150,\n",
    "                                             chunk_overlap=15,\n",
    "                                             length_function=len)\n",
    "  doc_list = textSplit.split_text(source)\n",
    "  return doc_list  \n",
    "\n",
    "\n",
    "def get_git_files(repo_link, folder_path, file_ext):\n",
    "  # eg. loading only python files\n",
    "  git_loader = GitLoader(clone_url=repo_link,\n",
    "    repo_path=folder_path, \n",
    "    file_filter=lambda file_path: file_path.endswith(file_ext))\n",
    "  #Will take each file individual document\n",
    "  git_docs = git_loader.load()\n",
    "\n",
    "  textSplit = RecursiveCharacterTextSplitter(chunk_size=150,\n",
    "                                             chunk_overlap=15,\n",
    "                                             length_function=len)\n",
    "  doc_list = []\n",
    "  #Pages will be list of pages, so need to modify the loop\n",
    "  for code in git_docs:\n",
    "    code_splits = textSplit.split_text(code.page_content)\n",
    "    doc_list.extend(code_splits)\n",
    "\n",
    "  return doc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def embed_index(doc_list, embed_fn, index_store):\n",
    "  \"\"\"Function takes in existing vector_store, \n",
    "  new doc_list and embedding function that is \n",
    "  initialized on appropriate model. Local or online. \n",
    "  New embedding is merged with the existing index. If no \n",
    "  index given a new one is created\"\"\"\n",
    "  #check whether the doc_list is documents, or text\n",
    "  try:\n",
    "    faiss_db = FAISS.from_documents(doc_list, \n",
    "                              embed_fn)  \n",
    "  except Exception as e:\n",
    "    faiss_db = FAISS.from_texts(doc_list, \n",
    "                              embed_fn)\n",
    "  \n",
    "  if os.path.exists(index_store):\n",
    "    local_db = FAISS.load_local(index_store,embed_fn)\n",
    "    #merging the new embedding with the existing index store\n",
    "    local_db.merge_from(faiss_db)\n",
    "    print(\"Merge completed\")\n",
    "    local_db.save_local(index_store)\n",
    "    print(\"Updated index saved\")\n",
    "  else:\n",
    "    faiss_db.save_local(folder_path=index_store)\n",
    "    print(\"New store created...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_docs_length(index_path, embed_fn):\n",
    "  test_index = FAISS.load_local(index_path,\n",
    "                              embeddings=embed_fn)\n",
    "  test_dict = test_index.docstore._dict\n",
    "  return len(test_dict.values())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing out the above function with the open source \n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=get_pdf_splits(\"The-Holy-Bible-King-James-Version.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40034\n",
      "The King James Version of the\n",
      "Holy Bible\n",
      "Downloaded from www.holybooks.com\n",
      "www.holybooks.com\n"
     ]
    }
   ],
   "source": [
    "print(len(docs))\n",
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge completed\n",
      "Updated index saved\n"
     ]
    }
   ],
   "source": [
    "faiss_db = embed_index(doc_list=docs,\n",
    "            embed_fn=embeddings,\n",
    "            index_store='new_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40055\n"
     ]
    }
   ],
   "source": [
    "print(get_docs_length(index_path='new_index',embed_fn=embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idex = FAISS.load_local(\"new_index\",embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Federal Capital Territory Administration\\nJul 2021 - Jun 2022 (1 year)\\nEngineering Intern\\nEducation\\nAfe Babalola University', metadata={}),\n",
       " Document(page_content='knowledge. {8:10} Receive my instruction, and not silver;\\nand knowledge rather than choice gold. {8:11} For wisdom', metadata={}),\n",
       " Document(page_content='increaseth learning. {16:22} Understanding [is] a wellspring\\nof life unto him that hath it: but the instruction of fools [is]', metadata={}),\n",
       " Document(page_content='learning; and a man of understanding shall attain unto wise\\ncounsels: {1:6} To understand a proverb, and the', metadata={})]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_idex.similarity_search(\"Education\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='him with the spirit of God, in wisdom, in understanding,\\nand in knowledge, and in all manner of workmanship;', metadata={}),\n",
       " Document(page_content='if any have caused grief, he hath not grieved me, but in part:', metadata={})]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Tell me his experience\"\n",
    "test_idex.max_marginal_relevance_search(question,k=2, fetch_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb=test_idex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "checkpoint = \"LaMini-T5-738M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    checkpoint,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype= torch.float32\n",
    ")\n",
    "\n",
    "\n",
    "pipe = pipeline(\n",
    "    'text2text-generation',\n",
    "    model = base_model,\n",
    "    tokenizer = tokenizer,\n",
    "    max_length = 900,\n",
    "    do_sample = True,\n",
    "    temperature = 0.3,\n",
    "    top_p = 0.95\n",
    ")\n",
    "local_llm= HuggingFacePipeline(pipeline=pipe)\n",
    "llm = local_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Who is adam?\"\n",
    "docs = vectordb.similarity_search(question,k=3)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Adam is a man who was first formed, then Eve.'"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = qa_chain({\"query\": question})\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Build prompt\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer. \n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The context does not provide information about his certifications.\n",
      "page_content='him with the spirit of God, in wisdom, in understanding,\\nand in knowledge, and in all manner of workmanship;' metadata={}\n"
     ]
    }
   ],
   "source": [
    "question = \"what are his certification\"\n",
    "result = qa_chain({\"query\": question})\n",
    "print(result[\"result\"])\n",
    "print(result[\"source_documents\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "retriever=vectordb.as_retriever()\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The context does not provide enough information to determine if it is helpful or not.\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\n",
    "result = qa({\"question\": question})\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jesus was told parables.\n"
     ]
    }
   ],
   "source": [
    "question = \"what were the parables Jesus told?\"\n",
    "result = qa({\"question\": question})\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
